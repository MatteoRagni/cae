#!/usr/bin/env python3

# Importing libraries
# For using my GPU I have to do that...
# export TF_MIN_GPU_MULTIPROCESSOR_COUNT=2

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import sys
sys.path.insert(0, './lib')

import pdb
import os
from timer import Timer
from timer import timestring
import json
from argparse import ArgumentParser
from data_handler import DataHandler
from autoenc_shell import ConvAutoEncShell

# PARSING COMMAND LINE ARGUMENTS
cmd = ArgumentParser(prog="caes-bin",
                     description="Stacked Convolutional Autoencoder",
                     epilog="Matteo Ragni, David Windridge - 2016")

DEFAULT_TRAINING_DIR  = '/tmp/training_nn'
CURRENT_TRAINING_TIME = timestring()
DEFAULT_RUNNING_DIR   = os.path.join(DEFAULT_TRAINING_DIR, CURRENT_TRAINING_TIME + '-run')
DEFAULT_SAVING_DIR    = os.path.join(DEFAULT_TRAINING_DIR, 'save-' + CURRENT_TRAINING_TIME + '-cae.ckpt')
DEFAULT_BATCH_SIZE    = 10
DEFAULT_STEP_SIZE     = 10
DEFAULT_LEARN_RATE    = 0.001
VERBOSITY             = 1
BATCH_LIMIT           = 4

# TODO: Modify run and training dir to add a single
# voice to define the path, and another to define the run.

cmd.add_argument('-in', dest='dataset', type=str, nargs=1, required=True,
                 help='Input dataset for the training')
cmd.add_argument('-sp', dest='support', type=str, nargs=1, required=True,
                 help='Input dataset as support file')
cmd.add_argument('-gpu', dest='gpu', action='store_true',
                 help='Enable CUDA and CuDNN [NO]')
cmd.add_argument('--cumulate-error', dest='cumulate_error', action='store_true',
                 help='When training different blocks accumulate errors of all blocks for training [NO]')
cmd.add_argument('-O', '--single-optimizer', dest='single_opt', action='store_true',
                 help='Runs the optimization using a single optimizer for the all structure [NO]')
cmd.add_argument('-r', '--run', dest='runs', type=str, nargs=1, default=DEFAULT_RUNNING_DIR,
                 help='Define a specific run directory [%s]' % DEFAULT_RUNNING_DIR)
cmd.add_argument('-s', '--save', dest='save', type=str, nargs=1, default=DEFAULT_SAVING_DIR,
                 help='NOT IMPLEMENTED - Define save positions for the result of the training, as checkpoint [%s]' % DEFAULT_SAVING_DIR)
cmd.add_argument('-bs', '--batchsize', dest='batchsize', type=int, nargs=1, default=DEFAULT_BATCH_SIZE,
                 help='Define the size of each batch. It must be compatible with RAM or VRAM capabilities. It must be positive [%d]' % DEFAULT_BATCH_SIZE)
cmd.add_argument('-ss', '--stepsize', dest='stepsize', type=int, nargs=1, default=DEFAULT_STEP_SIZE,
                 help='Define the number of step optimizer will run on each batch. It must be positive [%d]' % DEFAULT_STEP_SIZE)
cmd.add_argument('-lr', '--learnrate', dest='learnrate', type=float, nargs=1, default=DEFAULT_LEARN_RATE,
                 help='Define the learning rate for the optimizer [%f]' % DEFAULT_LEARN_RATE)
cmd.add_argument('-v', '--verbosity', dest='verb', type=int, nargs=1, default=VERBOSITY,
                 help='Define verbosity level, from 0 to 3 [%d]' % VERBOSITY)
cmd.add_argument('-rl', '--residual-learning', dest='residuals', action='store_true',
                 help='Enable residual learning (NO -> y = f(g(x)), YES -> y = f(g(x)) + x) [NO]')
cmd.add_argument('-tg', '--telegram', dest='telegram', action='store_true',
                 help='Enable notifications using system telegram bot [NO]')
cmd_args = cmd.parse_args()

# Handles the command line arguments and check everything is ok
try:
    assert os.path.isfile(' '.join(cmd_args.dataset)), "Dataset file [%s] does not exist" % ' '.join(cmd_args.dataset)
    assert os.path.isfile(' '.join(cmd_args.support)), "Support file [%s] does not exist" % ' '.join(cmd_args.support)
    assert cmd_args.stepsize  > 0,                     "Step size must be a positive integer"
    assert cmd_args.batchsize > 0,                     "Batch size must be a positive integer"
    assert cmd_args.learnrate  > 0.0,                   "Batch size must be a positive float"
except AssertionError as error:
    print("Input Error: " + str(error))
    exit(126)

if cmd_args.verb > 3:
    VERBOSITY = 3
elif cmd_args.verb < 0:
    VERBOSITY = 0

def batch_loop(length):
    VERBOSITY = cmd_args.verb[0]

# DEVICE_PLACEMENT_LOG = True if VERBOSITY > 1 else False
DEVICE_PLACEMENT_LOG = False

# Telegram notifications
if cmd_args.telegram:
    from tg_notifier import TelegramNotifier
    notifier = TelegramNotifier()
    def notify(m):
        notifier.post(m)
else:
    def notify(m):
        pass

# Logging function
def log(s, l=1):
    if l >= VERBOSITY:
        print(s)
    if l == 1:
        notify(str(s))

# MAIN PART
from six.moves import cPickle as pickle
import numpy as np
import tensorflow as tf
import autoencoder


# Some definitions on the engine
FLAGS = tf.app.flags.FLAGS
tf.app.flags.DEFINE_string('dataset_file',
                           cmd_args.dataset[0],
                           "Dataset file for training")
tf.app.flags.DEFINE_string('support_file',
                           cmd_args.support[0],
                           "Dataset file for support the training")
tf.app.flags.DEFINE_string('train_dir',
                           cmd_args.runs,
                           "Training directory")
tf.app.flags.DEFINE_string('saver_dir',
                           cmd_args.save,
                           "Chackpoint file for saving")
tf.app.flags.DEFINE_integer('max_steps',
                            cmd_args.stepsize,
                            "Number of steps for the optimizer on each batch")
tf.app.flags.DEFINE_integer('batch_size',
                            cmd_args.stepsize,
                            "Number of elements in each batch")
tf.app.flags.DEFINE_boolean('log_device_placement',
                            DEVICE_PLACEMENT_LOG,
                            "Logging of the use of my device")
tf.app.flags.DEFINE_boolean('gpu_enabled',
                            cmd_args.gpu,
                            "Enables the use of GPU for Convolutions")
tf.app.flags.DEFINE_boolean('cumulate_error',
                            cmd_args.cumulate_error,
                            "Enables the use of cumulative errors")
tf.app.flags.DEFINE_float('learning_rate',
                          cmd_args.learnrate,
                          "Optimizer learning rate")

#   ___           __ _                    _   _
#  / __|___ _ _  / _(_)__ _ _  _ _ _ __ _| |_(_)___ _ _
# | (__/ _ \ ' \|  _| / _` | || | '_/ _` |  _| / _ \ ' \
#  \___\___/_||_|_| |_\__, |\_,_|_| \__,_|\__|_\___/_||_|
#                     |___/

sets1 = autoencoder.ConvAutoEncSettings()
sets1.prefix_name = "alpha"
sets1.input_shape = [FLAGS.batch_size, 161, 161, 1]
sets1.corruption = False
sets1.layers = 1
sets1.patch_size = 5
sets1.strides = [1, 2, 2, 1]
sets1.padding = 'SAME'
sets1.depth_increasing = 4
sets1.residual_learning = cmd_args.residuals

sets2 = autoencoder.ConvAutoEncSettings()
sets2.prefix_name = "beta"
sets2.input_shape = [FLAGS.batch_size, 81, 81, 5]
sets2.corruption = False
sets2.layers = 1
sets2.patch_size = 5
sets2.strides = [1, 2, 2, 1]
sets2.padding = 'SAME'
sets2.depth_increasing = 4
sets2.residual_learning = cmd_args.residuals

sets3 = autoencoder.ConvAutoEncSettings()
sets3.prefix_name = "gamma"
sets3.input_shape = [FLAGS.batch_size, 41, 41, 9]
sets3.corruption = False
sets3.layers = 1
sets3.patch_size = 5
sets3.strides = [1, 2, 2, 1]
sets3.padding = 'SAME'
sets3.depth_increasing = 4
sets3.residual_learning = cmd_args.residuals

sets4 = autoencoder.ConvAutoEncSettings()
sets4.prefix_name = "delta"
sets4.input_shape = [FLAGS.batch_size, 21, 21, 13]
sets4.corruption = False
sets4.layers = 1
sets4.patch_size = 5
sets4.strides = [1, 2, 2, 1]
sets4.padding = 'SAME'
sets4.depth_increasing = 4
sets4.residual_learning = cmd_args.residuals

sets5 = autoencoder.ConvAutoEncSettings()
sets5.prefix_name = "epsilon"
sets5.input_shape = [FLAGS.batch_size, 11, 11, 17]
sets5.corruption = False
sets5.layers = 1
sets5.patch_size = 5
sets5.strides = [1, 2, 2, 1]
sets5.padding = 'SAME'
sets5.depth_increasing = 4
sets5.residual_learning = cmd_args.residuals

sets6 = autoencoder.ConvAutoEncSettings()
sets6.prefix_name = "zeta"
sets6.input_shape = [FLAGS.batch_size, 6, 6, 21]
sets6.corruption = False
sets6.layers = 1
sets6.patch_size = 5
sets6.strides = [1, 2, 2, 1]
sets6.padding = 'SAME'
sets6.depth_increasing = 4
sets6.residual_learning = cmd_args.residuals

sets = (
    sets1,
    sets2,
    sets3,
    sets4,
    sets5,
    sets6
)

#              _
#  ___ _ _  __| |
# / -_) ' \/ _` |
# \___|_||_\__,_|


#  _____       _
# |_   _|__ __| |_ ___
#   | |/ -_|_-<  _(_-<
#   |_|\___/__/\__/__/


#              _
#  ___ _ _  __| |
# / -_) ' \/ _` |
# \___|_||_\__,_|

# Check if there is a checkpoint to restore
RESTORE = os.path.isfile(FLAGS.saver_dir)

# Loading the Convolutional Autoencoder
stack = autoencoder.ConvAutoEncStack(sets)

merged = tf.merge_all_summaries()
writer = tf.train.SummaryWriter(FLAGS.train_dir, stack.graph)
counter = 0

# try:
dh = DataHandler(FLAGS.dataset_file, FLAGS.support_file, tuple(sets1.input_shape))
losses_string = ""
stack.writeConfiguration(os.path.join(FLAGS.train_dir, "config.txt"))

if cmd_args.single_opt:
    result = [None, 99999999.9999999]
    session = stack.session

    with tf.name_scope("TRAINING"):

        current_batch = -1
        for batch_no, dataset in dh.loop(FLAGS.batch_size, BATCH_LIMIT):

            if current_batch != batch_no:
                current_batch = batch_no
                log("RUNNING BATCH: %d (c: %i, e: %5.10f) on single layer" % (current_batch, counter, result[1]))

            with Timer():  # batch timing

                for step in range(0, FLAGS.max_steps):
                    result = session.run([stack.optimizer], feed_dict={stack.caes[0].x: dataset[0]})
                    counter += 1

                # Handling the losses at the end of an optimization run
                losses = 0
                loss_string = "%d" % counter
                for im in dataset:
                    loss = session.run(stack.error, feed_dict={stack.caes[0].x: im})
                    loss_string += "\t%f" % loss
                    losses += loss
                loss_string += "\t%f\n" % losses
                log("Loss report => " + loss_string)
                losses_string += loss_string

                result = session.run([merged, stack.error], feed_dict={stack.caes[0].x: dataset[0]})
                log("Error at step %i is: %5.10f" % (counter, result[1]), 2)
                writer.add_summary(result[0], counter)

else:
    for session, n, cae, x in stack.trainBlocks():
        result = [None, 99999999.9999999]

        with tf.name_scope("TRAINING-%d" % n):

            current_batch = -1
            for batch_no, dataset in dh.loop(FLAGS.batch_size, BATCH_LIMIT):

                if current_batch != batch_no:
                    current_batch = batch_no
                    log("RUNNING BATCH: %d (c: %i, e: %5.10f) on layer %d" % (current_batch, counter, result[1], n))

                with Timer():  # batch timing

                    for step in range(0, FLAGS.max_steps):
                        result = session.run([cae.optimizer], feed_dict={x: dataset[0]})
                        counter += 1

                    # Handling the losses at the end of an optimization run
                    losses = 0
                    loss_string = "%d" % counter
                    for im in dataset:
                        loss = session.run(stack.error, feed_dict={x: im})
                        loss_string += "\t%f" % loss
                        losses += loss
                    loss_string += "\t%f\n" % losses
                    log("Loss report => " + loss_string)
                    losses_string += loss_string

                    result = session.run([merged, cae.error], feed_dict={x: dataset[0]})
                    log("Error at step %i is: %5.10f" % (counter, result[1]), 2)
                    writer.add_summary(result[0], counter)


with open(os.path.join(FLAGS.train_dir, "loss_report.csv"), "w") as f:
    f.write(losses_string)

# from autoenc_shell import ConvAutoEncShell
ConvAutoEncShell({
    "objects": 3,
    "positions": 25,
    "session": session,
    "stack": stack,
    "counter": counter,
    "writer": writer,
    "dh": dh,
    "summary": merged
}).cmdloop()

# finally:
#   stack.close()
log("Learning Completed")
notify("Learning completed")
exit(0)
